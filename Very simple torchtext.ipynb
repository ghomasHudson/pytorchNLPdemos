{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchtext example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handy Links\n",
    "- https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb\n",
    "- http://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/\n",
    "- http://anie.me/On-Torchtext/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have already preprocessed the data slightly so we have 3 files:\n",
    "- train.csv\n",
    "- test.csv\n",
    "- classes.csv\n",
    "\n",
    "train.csv and test.csv are the standard format of the label followed by the text:\n",
    "```\n",
    "LABEL1,TEXT1\n",
    "LABEL2,TEXT2\n",
    "...\n",
    "```\n",
    "The label is just 0 or 1 (pos, neg)\n",
    "\n",
    "Lets store the path to our data folder in a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"aclImdb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the libraries we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fields\n",
    "ok good. We can now start processing our data.\n",
    "\n",
    "We need 2 **Fields**: one for the label and one for the text itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "REVIEW = data.Field(sequential=True,lower=True,tokenize=\"spacy\")\n",
    "LABEL = data.LabelField(use_vocab=False,dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These store what preprocessing (tokenisation - splitting into words, and numerisation - converting these words into numbers) will be done on each string, but doesn't do it until we pass the data in.\n",
    "\n",
    "For more details see the torchtext docs on [Field](https://torchtext.readthedocs.io/en/latest/data.html#field).\n",
    "\n",
    "You might want to also consider the **ReversibleField** class. This uses a different tokeniser: [revtok](https://github.com/jekbradbury/revtok) which allows you to map back to strings. This can be handy, especially when debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Once we have the fields defined, we can initialise a **Dataset**.\n",
    "\n",
    "This class does the heavy lifting of actually loading the data into these fields.\n",
    "\n",
    "If your data is in some weird format you can implement your own class (it just has to match the [interface](https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.Dataset). There are loads of [examples](https://github.com/pytorch/text/tree/master/torchtext/datasets)!).\n",
    "\n",
    "Here as we have CSV files we're just gonna use the handy TabularDataset class already provided for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 25000\n",
      "Test set size : 25000\n"
     ]
    }
   ],
   "source": [
    "train, test = data.TabularDataset.splits(\n",
    "    path=PATH,\n",
    "    format=\"csv\",\n",
    "    train = \"train.csv\",\n",
    "    test = \"test.csv\",\n",
    "    fields=[('label',LABEL),('review',REVIEW)])\n",
    "\n",
    "print(\"Train set size:\",len(train))\n",
    "print(\"Test set size :\",len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us 2 Datasets: train and test.\n",
    "\n",
    "NOTE1: We should really add a validation set in as well.\n",
    "\n",
    "NOTE2: Loading and preprocessing takes time. We should pickle these objects so we only have to do it once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torchtext will now tokenize our data based on what we told it to do in our fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['by', 'my', '\"', 'kool', '-', 'aid', 'drinkers', '\"']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.examples[0].review[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load in some pretrained word embeddings into our text field using the build_vocab method of the text field.\n",
    "\n",
    "We pass the name of the embedding file. This can be an actual file on your computer or one of the [predefined names](https://torchtext.readthedocs.io/en/latest/vocab.html#pretrained-aliases) which includes common embeddings such as word2vec, GloVe and FastText. If you pick one of these torchtext will go and download these for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "REVIEW.build_vocab(train, vectors=\"glove.6B.100d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have these vectors downloaded and stored inside the Field object, we need to create the mapping from our tokenised words to these word vectors.\n",
    "\n",
    "PyTorch has an nn.Embedding layer which will do this job, and we will include it in our model. Here we define a function which we will call to set up this nn.Embedding layer (see the model definition later for where we call it):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb(textField,em_sz=300):\n",
    "    \"\"\"Create embedding matrix from GloVe\"\"\"\n",
    "    emb = nn.Embedding(len(textField.vocab.itos),em_sz,padding_idx=1)\n",
    "    weights = emb.weight.data\n",
    "    miss = []\n",
    "    for i,w in  enumerate(textField.vocab.itos):\n",
    "        try: weights[i] = torch.from_numpy(textField.vocab.vectors[w]*3)\n",
    "        except: miss.append(w)\n",
    "    print(\"OOV:\",len(miss),miss[5:10]) # just to check\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in the text field object and loops through every word in the vocabulary \n",
    "(the property itos - Integer To String, maps from integers back to the strings ).\n",
    "\n",
    "On each loop iteration i, we set the ith row of the embedding weight matrix to the word vector (\\*3 to give us space for special tokens we might want to add such as OOV,EOS etc...)\n",
    "\n",
    "If the word doesn't have a word vector we skip it and add it to a list of misses to help us debug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interators\n",
    "The last piece of the preprocessing puzzle is the **Iterator**. \n",
    "This class tells torchtext how to loop over the data in **batches**.\n",
    "\n",
    "We use the handy BucketIterator here which groups similar length reviews together meaning we don't need as much padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if we can use the GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "train_iter,test_iter = data.BucketIterator.splits(\n",
    "    (train,test),\n",
    "    batch_sizes=(64,64),\n",
    "    sort_key=lambda x: len(x.review),\n",
    "    device=device #Run on the GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually test out these iterators, getting the next item as you would with a standard [Python iterator](https://www.geeksforgeeks.org/iterators-in-python/). This is exactly what our model is going to do: looping over the iterator, grabbing new batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['batch_size', 'dataset', 'fields', 'input_fields', 'target_fields', 'label', 'review'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[13110,    20,    72],\n",
       "        [    9,    97, 26785],\n",
       "        [  780,  1983,  1610],\n",
       "        [    6,    35,    56],\n",
       "        [   85,  3299,    13]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "print(vars(batch).keys())\n",
    "batch.review[:5, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Model\n",
    "Now it's time to write a simple model to classify the reviews. We have 3 layers: an Embedding, an LSTM and finally a Linear output layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV: 101867 ['and', 'a', 'of', 'to', 'is']\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class SimpleLSTMmodel(nn.Module):\n",
    "    def __init__(self,textField,hidden_dim=1000,emb_dim=300):\n",
    "        \"\"\"Initialise all the layers\"\"\"\n",
    "        super().__init__() #very important!\n",
    "        \n",
    "        self.embedding = create_emb(textField,emb_dim) #calling the function from earlier!\n",
    "        self.rnn = nn.LSTM(emb_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    \n",
    "    def forward(self,seq):\n",
    "        \"\"\"Connect all the layers together\"\"\"\n",
    "        embedded = self.embedding(x) #embedded = [sent len, batch size, emb dim]\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        hidden = hidden[-1]\n",
    "        return self.fc(hidden.squeeze(0))\n",
    "\n",
    "model = SimpleLSTMmodel(REVIEW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When writing a NN (or even a single layer) in PyTorch we always have these 2 methods: **\\_\\_init\\_\\_** where we define what layers we want, and **forward** where we define how the sequence gets passed through these layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now we have our model we need to train it for our task: getting the sentiment of movie reviews.\n",
    "\n",
    "Here's a fairly standard training loop which does the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e35280deed4574bd23de510093129d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=391), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-819ec8b0a5de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#make a prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#find the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# backpropogate this loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# make a step of the parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pyTorch/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pyTorch/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "from tqdm import  tqdm_notebook as tqdm\n",
    "opt = optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "#if we have a gpu move our stuff there\n",
    "model = model.to(device)\n",
    "loss_func = loss_func.to(device)\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    \n",
    "    #***********\n",
    "    # Train\n",
    "    train_loss = 0\n",
    "    model.train() #allow the model to train\n",
    "    \n",
    "    for b in tqdm(train_iter): #train a mini-batch\n",
    "        x, y = b.review, b.label\n",
    "        opt.zero_grad()\n",
    "        preds = model(x).squeeze(1) #make a prediction\n",
    "        loss = loss_func(preds,y) #find the loss\n",
    "        loss.backward() # backpropogate this loss\n",
    "        opt.step() # make a step of the parameters\n",
    "        \n",
    "        train_loss += loss.item()  \n",
    "    #***********\n",
    "    # Validate\n",
    "    val_loss = 0\n",
    "    model.eval() #switch to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for b in test_iter:\n",
    "            x, y = b.review, b.label\n",
    "            preds = model(x).squeeze(1)\n",
    "            loss = loss_func(preds,y)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    print('(Epoch',str(epoch)+\"/\"+str(epochs)+\")\",\n",
    "          \"train loss:\",train_loss/len(train_iter),\n",
    "          \"val loss:\",val_loss/len(test_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: fast.ai simplifies this by having a standard training loop you can use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting\n",
    "Now we can make some predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc627ef962eb44a0b356dde1f6ac0275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=391), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 predictions:\n",
      " [ 0.06850207  0.79389507 -1.58640885  1.27722037 -1.15759289]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "preds = np.array([])\n",
    "y_test = np.array([])\n",
    "for b in tqdm(test_iter):\n",
    "    x, y = b.review, b.label\n",
    "    predsTmp = model(x).cpu().squeeze(1).data.numpy()\n",
    "    y = y.cpu().data.numpy()\n",
    "    preds = np.concatenate((preds,predsTmp))\n",
    "    y_test = np.concatenate((y_test,y))\n",
    "    \n",
    "print(\"First 5 predictions:\\n\",preds[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then calculate accuracy, F1-measure and anything else you could dream of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "\n",
      "Accuracy 0.5204\n"
     ]
    }
   ],
   "source": [
    "def accuracy_score(preds,y):\n",
    "    return np.mean(preds==y)\n",
    "\n",
    "preds = np.where(preds < 0.5,0.0,1.0)\n",
    "print(preds)\n",
    "print(y_test)\n",
    "print()\n",
    "print(\"Accuracy\",accuracy_score(preds,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "And we're done! This model could be a lot more complicated, with more layers, maybe a biLSTM etc etc. Feel free to tweak the model class.\n",
    "\n",
    "Check out the [fastai libary](https://github.com/fastai/fastai) which makes many of these steps much easier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
